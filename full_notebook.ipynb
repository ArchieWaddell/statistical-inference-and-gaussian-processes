{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Inference and Gaussian Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Statistical Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Bayesian vs. Frequentist Modelling\n",
    "\n",
    "Here I compare two statistical paradigms—**frequentist inference** and **Bayesian inference**—on the problem of estimating a coin’s probability of landing heads. The dataset `coin_experiments.csv` contains 1,000 independent coin-flip experiments, each recording the number of **successes** (heads) and the total **trials**.\n",
    "\n",
    "I first treat the probability of success, $p$, as a fixed but unknown quantity (frequentist view). I:\n",
    "- Import the data and run basic sanity checks (shape, missing values, and ensuring successes ≤ trials).\n",
    "- Compute the **Maximum Likelihood Estimate (MLE)**, $\\hat{p} = S/N$, where $S$ is the total number of successes and $N$ is the total number of trials.\n",
    "- Estimate the standard error\n",
    "\n",
    "$$\n",
    "\\text{SE} = \\sqrt{\\hat{p}(1-\\hat{p})/N},\n",
    "$$\n",
    "\n",
    "and form a **95% Wald confidence interval** using $z = 1.96$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Frequentist estimation (MLE + Wald interval)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (3.13.7) (Python 3.13.7)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/Users/archie/Documents/MSc_modules/semester_1/reasoning_under_uncertainty/statistical-inference-and-gaussian-processes/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "rng = np.random.default_rng(0) # Set seed for consistency with drawing from distributions\n",
    "\n",
    "# Read data\n",
    "coin_df = pd.read_csv('datasets_part_1/coin_experiments.csv')\n",
    "print(f\"Shape: {coin_df.shape}\") # check shape\n",
    "print(f\"Null counts:\\n{coin_df.isna().sum().to_string()}\") # null counts\n",
    "\n",
    "invalid_trials = sum(coin_df['successes']>coin_df['trials'])\n",
    "print(f\"Number of invalid trials (n_successes > n_trials): {invalid_trials}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute quantities for MLE\n",
    "S = coin_df['successes'].sum()\n",
    "N = coin_df['trials'].sum()\n",
    "\n",
    "mle = S / N\n",
    "\n",
    "def compute_se(total_trials, p_hat, z):\n",
    "    \"\"\"\n",
    "    Returns standard error and Wald confidence interval bounds for a Binomial proportion estimate.\n",
    "    \"\"\"\n",
    "    se = np.sqrt(p_hat * (1 - p_hat) / total_trials)\n",
    "    confidence_interval = (p_hat - z * se, p_hat + z * se)\n",
    "    return se, confidence_interval\n",
    "\n",
    "se, confidence_interval = compute_se(N, mle, z=1.96)\n",
    "\n",
    "print(rf'Maximum Likelihood Estimator (MLE) = {mle:.6f}')\n",
    "print(f'Standard Error (SE) = {se:.6f}')\n",
    "print(f'Wald Confidence Interval (95%): [{float(confidence_interval[0]):.6f}, {float(confidence_interval[1]):.6f}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I switch to the Bayesian view and treat $p$ as a **random variable** with a prior distribution. Using a **Beta(1, 1)** prior (uniform), I:\n",
    "- Compute the posterior parameters\n",
    "\t$$\n",
    "\t\\alpha_{\\text{post}} = \\alpha_0 + S, \\quad \\beta_{\\text{post}} = \\beta_0 + (N-S),\n",
    "\t$$\n",
    "\tand the **MAP estimate**\n",
    "\t$$\n",
    "\tp_{\\text{MAP}} = \\frac{\\alpha_{\\text{post}} - 1}{\\alpha_{\\text{post}} + \\beta_{\\text{post}} - 2}.\n",
    "\t$$\n",
    "- Draw 5,000 samples from the Beta posterior for downstream posterior-predictive simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian updating (Beta prior → Beta posterior) + posterior sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import beta\n",
    "\n",
    "alpha_prior, beta_prior = 1, 1 # flat prior\n",
    "\n",
    "alpha_post = alpha_prior + S\n",
    "beta_post = beta_prior + (N - S)\n",
    "\n",
    "p_map = (alpha_post - 1)/(alpha_post + beta_post - 2)\n",
    "\n",
    "print(f\"Posterior alpha = {alpha_post}\")\n",
    "print(f\"Posterior beta = {beta_post}\")\n",
    "print(f\"Maximum A Posteriori (MAP) = {p_map:.6f}\")\n",
    "\n",
    "# Draw 5000 samples from posterior\n",
    "samples = beta.rvs(alpha_post, beta_post, size=5000, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I compare predictions for the next **20 flips**:\n",
    "\n",
    "- For the Bayesian model, I simulate the posterior predictive by sampling outcomes from **Binomial(20, p)** where $p$ varies across posterior samples, then visualise the distribution.\n",
    "\n",
    "- For the frequentist model, I plot the **plug-in Binomial pmf** with $p = \\hat{p}_{\\text{MLE}}$ on the same figure for direct comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posterior predictive vs plug-in predictive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "# Draw Binomial(20,p) for each posterior sample p in `samples`\n",
    "bayes_predictions = binom.rvs(20, samples, random_state=rng)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "sns.histplot(bayes_predictions, \n",
    "            bins = np.arange(-0.5, 20.5, 1),\n",
    "            stat='probability', \n",
    "            label='Bayesian posterior predictive',\n",
    "            ax=ax)\n",
    "\n",
    "# Frequentist (plug-in) model - one p value\n",
    "n = 20\n",
    "p_hat = mle\n",
    "k = np.arange(0, n+1)\n",
    "pmf = binom.pmf(k, n, p_hat) # probability mass function\n",
    "\n",
    "ax.plot(\n",
    "    k, pmf,\n",
    "    'o-', \n",
    "    color=sns.color_palette()[1],\n",
    "    label='Frequentist MLE predictive',\n",
    "    markersize=6\n",
    ")\n",
    "\n",
    "# Plotting parameters\n",
    "ax.set_xlabel('Number of heads in 20 flips')\n",
    "ax.set_ylabel('Probability')\n",
    "ax.set_title('Predictive distribution - Bayesian vs Frequentist')\n",
    "ax.set_xticks(np.arange(0, 21))\n",
    "ax.set_xlim(-0.5, 20.5)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "\n",
    "Both methods produce almost the same curve here because the data make the estimate of the coin’s bias extremely precise. The frequentist (plug-in) approach treats the estimate (MLE) as if it were the true parameter, so its predictions come straight from a single binomial model. The Bayesian version mixes over many nearby parameter values, but when those values are tightly clustered, i.e. a narrow posterior, the mixture collapses to something that looks nearly identical to the frequentist approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Variance Reduction with Importance Sampling (5 marks)\n",
    "\n",
    "Many integrals that arise in statistics and machine learning cannot be evaluated analytically, especially in higher dimensions. **Monte Carlo (MC) integration** is a general technique to approximate such integrals by drawing random samples from a distribution and averaging function evaluations. Although MC estimators are unbiased, their variance can be large, meaning we may need many samples to reach a desired accuracy.  \n",
    "\n",
    "A way to improve efficiency is through **importance sampling (IS)**. Instead of sampling from a fixed distribution (e.g., Gaussian), we choose a proposal distribution $q(X_{train})$ that better matches the regions where the integrand contributes most. By reweighting samples, we can reduce variance while keeping the estimator unbiased. The effectiveness of IS depends critically on choosing $q(X_{train})$ with heavier tails or shapes aligned with the integrand.  \n",
    "\n",
    "In this exercise, I evaluate a 2D integral and demonstrate how importance sampling can achieve significant variance reduction compared to plain Monte Carlo:  \n",
    "\n",
    "$$\n",
    "I = \\int_{\\mathbb{R}^2} \\exp(-\\|x\\|_1)\\,\\sin(\\|x\\|_2)\\,dx,\n",
    "$$  \n",
    "\n",
    "with the goal of achieving an **absolute error < 0.01**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the integrand\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def integrand(x):\n",
    "    l1 = np.sum(np.abs(x), axis=-1)   # L1 norm\n",
    "    l2 = np.linalg.norm(x, axis=-1)   # L2 norm\n",
    "    return np.exp(-l1) * np.sin(l2)\n",
    "\n",
    "# grid\n",
    "xx, yy = np.meshgrid(np.linspace(-4,4,400), np.linspace(-4,4,400))\n",
    "pts = np.stack([xx,yy], axis=-1)\n",
    "zz = integrand(pts)\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.pcolormesh(xx, yy, zz, cmap=\"RdBu_r\", shading=\"auto\")\n",
    "plt.colorbar(label=\"f(x)\")\n",
    "plt.title(\"Integrand $f(x) = e^{||x||_1} \\\\, \\\\sin(||x||_2)$\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain Monte Carlo\n",
    "\n",
    "I estimate $I$ by sampling $x \\sim \\mathcal{N}(0, I_2)$ and using the plain Monte Carlo estimator\n",
    "$$\n",
    "\\hat{I} = \\frac{1}{N} \\sum_{i=1}^N f(x_i), \\quad x_i \\sim \\mathcal{N}(0, I_2).\n",
    "$$\n",
    "I also compute the empirical standard error and check how it scales as $N$ increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plain Monte Carlo estimator + error scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimating I with plain Monte Carlo\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "rng = np.random.default_rng(1) # New random seed for different task\n",
    "\n",
    "N_plain = int(1e4) # Set value of samples to use in plain and IS MC\n",
    "mean = np.zeros(2)\n",
    "cov = np.eye(2)\n",
    "normal_dist = multivariate_normal(mean=mean, cov=cov) # standard multivariate (2D) normal\n",
    "\n",
    "def estimate_integrand_plain(N): \n",
    "    \"\"\"\n",
    "    Monte Carlo estimate of E[f(X)], and its estimator variance, using N draws from the \n",
    "    2D standard normal.\n",
    "    \"\"\"\n",
    "    samples = normal_dist.rvs(size=N, random_state=rng) # draw N i.i.d. samples\n",
    "    f_vals = integrand(samples) # evaluate integrand\n",
    "    estimate = np.mean(f_vals) # MC estimator\n",
    "    point_variance = np.var(f_vals, ddof=1) # sample variance of f(x)\n",
    "    estimator_variance = point_variance / N # variance of mean estimator\n",
    "    return estimate, estimator_variance\n",
    "\n",
    "estimate_plain, estimator_variance_plain = estimate_integrand_plain(N_plain)\n",
    "se_plain = np.sqrt(estimator_variance_plain) # standard error of estimator\n",
    "\n",
    "print(f\"Estimate of integral (plain MC) = {estimate_plain:.6f}\")\n",
    "print(f\"Standard empirical error = {se_plain:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verifying that error decreases as N increases\n",
    "\n",
    "N_vals = np.arange(1000, int(1e5), 1000) # Sample sizes to test\n",
    "se_vals_plain = np.zeros(len(N_vals))\n",
    "inv_sqrt_N = 1 / np.sqrt(N_vals)\n",
    "\n",
    "for i, N_val in enumerate(N_vals):\n",
    "    _, variance = estimate_integrand_plain(N_val)\n",
    "    se_vals_plain[i] = np.sqrt(variance) # Store SE\n",
    "\n",
    "# Plot error decreasing with N\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(N_vals, se_vals_plain)\n",
    "axes[0].set_title(\"Monte Carlo Standard Error vs Sample Size\")\n",
    "axes[0].set_xlabel(\"N\")\n",
    "axes[0].set_ylabel(\"Standard Error\")\n",
    "\n",
    "# Show linearity of SE with 1/root(N)\n",
    "axes[1].plot(inv_sqrt_N, se_vals_plain)\n",
    "axes[1].set_title(r\"Standard Error vs $1/\\sqrt{N}$\")\n",
    "axes[1].set_xlabel(r\"$1 / \\sqrt{N}$\")\n",
    "axes[1].set_ylabel(\"Standard Error\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling\n",
    "\n",
    "To improve efficiency, I design a proposal distribution $q(x)$ and implement an importance sampling estimator. I then compare variance against the plain Monte Carlo estimator for the same number of samples.\n",
    "\n",
    "I use a Student-t proposal family and tune its parameters (scale and degrees of freedom) via a simple grid search to reduce the estimator’s standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importance sampling estimator + proposal tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to estimate I using importance sampling MC\n",
    "\n",
    "def estimate_integrand_is(N, target_dist, proposal_dist):\n",
    "    \"\"\"\n",
    "    Importance-sampling estimate of E[f(X)] and variance using N proposal draws.\n",
    "    \"\"\"\n",
    "    samples = proposal_dist.rvs(size=N, random_state=rng) # draw from proposal\n",
    "    f_vals = integrand(samples)\n",
    "    target_pdf_vals = target_dist.pdf(samples) # target density\n",
    "    proposal_pdf_vals = proposal_dist.pdf(samples) # proposal density\n",
    "\n",
    "    # Clip proposal mass away from zero to avoid exploding IS weights when q(x) is tiny\n",
    "    weights = target_pdf_vals / np.clip(proposal_pdf_vals, a_min=1e-12, a_max=None) # importance weights\n",
    "    weighted_vals = f_vals * weights\n",
    "\n",
    "    estimate = np.mean(weighted_vals) # IS estimator\n",
    "    point_variance = np.var(weighted_vals, ddof=1) # variance of weights*f\n",
    "    estimator_variance = point_variance / N # variance of estimator\n",
    "    \n",
    "    return estimate, estimator_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid-search student-t proposal parameters (scale, df) for minimum standard error\n",
    "# Gives a \"well-selected\" prior\n",
    "\n",
    "from scipy.stats import multivariate_t\n",
    "\n",
    "min_se_is = np.inf\n",
    "best_scale, best_df = None, None\n",
    "\n",
    "for scale in np.arange(0.1, 2, 0.05):\n",
    "    for df in np.arange(1, 100, 2):\n",
    "        t_dist = multivariate_t(loc=mean, shape=scale*cov, df=df)\n",
    "        is_estimate, is_var = estimate_integrand_is(N_plain, normal_dist, t_dist)\n",
    "        is_se = np.sqrt(is_var)\n",
    "\n",
    "        # Save best params\n",
    "        if is_se < min_se_is:\n",
    "            min_se_is, best_var_is, best_est_is, best_scale, best_df = is_se, is_var, is_estimate, scale, df\n",
    "            t_dist_optimised = multivariate_t(loc=mean, shape=scale*cov, df=df)\n",
    "\n",
    "print(\"Student-t poposal distribution:\")\n",
    "print(f\"Best scale: {best_scale:.2f}, Best df: {best_df}\")\n",
    "print(f\"Estimate: {best_est_is:.6f}, SE: {min_se_is:.6f}\")\n",
    "\n",
    "# Show variance reduction for same N\n",
    "print(f\"Variance ratio (IS/MC): {best_var_is/estimator_variance_plain:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verifying that error decreases as N increases\n",
    "\n",
    "se_vals_is = np.zeros(len(N_vals))\n",
    "\n",
    "for i, N_val in enumerate(N_vals): # Same range as plain MC plot for comparison\n",
    "    _, variance = estimate_integrand_is(N_val, normal_dist, t_dist_optimised)\n",
    "    se_vals_is[i] = np.sqrt(variance)\n",
    "\n",
    "# Plot error decreasing with N\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(N_vals, se_vals_is)\n",
    "axes[0].set_title(\"Monte Carlo Standard Error vs Sample Size\")\n",
    "axes[0].set_xlabel(\"N\")\n",
    "axes[0].set_ylabel(\"Standard Error\")\n",
    "\n",
    "# Show linearity of SE with 1/root(N)\n",
    "axes[1].plot(inv_sqrt_N, se_vals_is)\n",
    "axes[1].set_title(r\"Standard Error vs $1/\\sqrt{N}$\")\n",
    "axes[1].set_xlabel(r\"$1 / \\sqrt{N}$\")\n",
    "axes[1].set_ylabel(\"Standard Error\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSE comparison across sample sizes\n",
    "\n",
    "I compare plain Monte Carlo and importance sampling using RMSE vs. sample size $N$ on a log–log plot. To do this, I compute a high-precision reference estimate and treat it as a baseline for error calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RMSE vs sample size (log–log)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish high-precision baseline so RMSE curves have a stable reference\n",
    "N_ref = int(1e7)\n",
    "I_ref, _ = estimate_integrand_plain(N_ref)\n",
    "\n",
    "# Sample sizes and number of independent runs\n",
    "N_vals = np.logspace(3, 5, num=10, dtype=int)\n",
    "M = 100 # Average over 100 independent runs per N to smooth Monte Carlo noise\n",
    "\n",
    "rmse_plain_vals = np.zeros(len(N_vals))\n",
    "rmse_is_t_vals = np.zeros(len(N_vals))\n",
    "\n",
    "# Outer loop sweeps sample sizes, inner loops collect estimates then compute RMSE vs reference\n",
    "for i, N_val in enumerate(N_vals):\n",
    "    # Plain MC\n",
    "    I_estimates_plain = np.zeros(M)\n",
    "    for m in range(M):\n",
    "        I_est_plain, _ = estimate_integrand_plain(N_val)\n",
    "        I_estimates_plain[m] = I_est_plain\n",
    "    rmse_plain_vals[i] = np.sqrt(np.mean((I_estimates_plain - I_ref)**2))\n",
    "\n",
    "    # Importance Sampling\n",
    "    I_estimates_is = np.zeros(M)\n",
    "    for m in range(M):\n",
    "        I_est_is, _ = estimate_integrand_is(N_val, normal_dist, t_dist_optimised)\n",
    "        I_estimates_is[m] = I_est_is\n",
    "    rmse_is_t_vals[i] = np.sqrt(np.mean((I_estimates_is - I_ref)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot RMSE vs. N (log–log)\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(N_vals, rmse_plain_vals, 'o-', label='Plain Monte Carlo', markersize=6)\n",
    "ax.plot(N_vals, rmse_is_t_vals, 'o-', label='Importance Sampling (Student-t)', markersize=6)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Sample Size (N)', fontsize=12)\n",
    "ax.set_ylabel('RMSE', fontsize=12)\n",
    "ax.set_title('RMSE of Integral Estimate vs Sample Size', fontsize=14)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show absolute error < 0.01 for both methods\n",
    "abs_err_plain = abs(estimate_plain - I_ref)\n",
    "abs_err_is = abs(best_est_is - I_ref)\n",
    "print(f\"|I_plain - I_ref| = {abs_err_plain:.4f}\")\n",
    "print(f\"|I_IS - I_ref| = {abs_err_is:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Justification**\n",
    "\n",
    "I selected the Student-t family because its tail behaviour can be adjusted to match the decay of the integrand. A proposal that decays too quickly risks missing regions that still contribute, leading to unstable importance weights. Student-t avoids this by allowing controlled tail-heaviness through the degrees of freedom.\n",
    "\n",
    "The optimisation then identified a high df as the most effective choice, approaching the Gaussian limit. This shows that, once the tail-width was allowed to adapt, the best match to the integrand’s decay happened to be the light-tailed case. The Student-t family provided the flexibility to reach that outcome without assuming it in advance. In practice, the optimised proposal aligned well with the integrand’s decay, stabilised the weights, and reduced the variance by roughly a factor of three for the same sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Mean-Field Variational Inference for Bayesian Logistic Regression\n",
    "\n",
    "In this exercise, I implement **Bayesian logistic regression** and approximate its posterior distribution using **mean-field variational inference (VI)**. I then compare VI with a **Hamiltonian Monte Carlo (HMC)** benchmark to highlight the trade-offs between computational efficiency and posterior accuracy. The dataset `log_reg_data.csv` contains predictors `x1, x2, …` and a binary label `y`.\n",
    "\n",
    "### 2.3.1. Background & Motivation\n",
    "\n",
    "- **Bayesian logistic regression** models uncertainty in the regression weights $w$, allowing us to quantify predictive uncertainty rather than relying only on a point estimate (as in standard logistic regression). The prior is chosen as a Gaussian:  \n",
    "  $$\n",
    "  w \\sim \\mathcal{N}(0, 10I).\n",
    "  $$  \n",
    "\n",
    "- **Variational Inference (VI)** approximates the true posterior $p(w \\mid D)$ by a simpler distribution. Here, we use a **mean-field Gaussian**:  \n",
    "  $$\n",
    "  q_\\phi(w) = \\mathcal{N}(w \\mid \\mu, \\mathrm{diag}(\\sigma^2)),\n",
    "  $$  \n",
    "  where the parameters $\\phi = (\\mu, \\sigma)$ are optimised to make $q_\\phi$ close to the true posterior.  \n",
    "\n",
    "- Optimisation is done via the **Evidence Lower Bound (ELBO)**:  \n",
    "  $$\n",
    "  \\mathcal{L}(\\phi) = \\mathbb{E}_{q_\\phi}[ \\log p(D \\mid w) ] - \\mathrm{KL}(q_\\phi \\,\\|\\, p(w)),\n",
    "  $$  \n",
    "  which we estimate stochastically using the **reparameterisation trick**:  \n",
    "  $$\n",
    "  w = \\mu + \\sigma \\odot \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, I).\n",
    "  $$  \n",
    "\n",
    "- **Hamiltonian Monte Carlo (HMC)** provides a high-fidelity posterior approximation by simulating from the exact Bayesian posterior using gradient information. It is computationally more expensive but is often treated as the “gold standard” for comparison.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational inference (mean-field Gaussian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autograd import grad\n",
    "import autograd.numpy as anp\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**VI implementation + ELBO training curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data, check shape and nulls\n",
    "log_reg_df = pd.read_csv('datasets_part_1/log_reg_data.csv')\n",
    "print(f\"Shape: {log_reg_df.shape}\")\n",
    "print(f\"Null counts:\\n{log_reg_df.isna().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split columns\n",
    "X = log_reg_df.drop(columns=['y']).values\n",
    "y = log_reg_df['y'].values\n",
    "\n",
    "# Add bias column\n",
    "bias = np.ones((X.shape[0], 1))\n",
    "X = np.hstack([bias, X])\n",
    "D = X.shape[1]\n",
    "\n",
    "# Train test split for 1.3.C\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.scipy.special import expit\n",
    "\n",
    "# Prior (Multivariate Gaussian Distribution)\n",
    "mu_prior = np.zeros(D)  # Prior mean for mu\n",
    "cov_scale = 10.0\n",
    "cov_prior = cov_scale*np.eye(D)  # Prior covariance matrix\n",
    "\n",
    "# Variational Parameters (Posterior Approximation)\n",
    "mu_q = np.zeros(D)  # Initial mean estimate for variational posterior\n",
    "log_cov_q = np.log(np.ones(D) * 0.1)  # Initial log of covariance matrix\n",
    "\n",
    "# New task, new random seed\n",
    "vi_seed = 1234\n",
    "vi_rng = np.random.default_rng(vi_seed)\n",
    "\n",
    "# Define elbo function\n",
    "def elbo(mu_q, log_cov_q, epsilon, rng, num_kl_samples=400):\n",
    "    \"\"\"\n",
    "    Stochastic ELBO for Bayesian logistic regression with diagonal Gaussian q(w).\n",
    "    \"\"\"\n",
    "    # Reparameterisation trick\n",
    "    # anp for autograd\n",
    "    sigma_q = anp.exp(0.5*log_cov_q)\n",
    "    w = mu_q + sigma_q * epsilon\n",
    "    logits = anp.dot(X_train, w)\n",
    "    \n",
    "    # Log likelihood\n",
    "    # Keep probabilities away from 0/1 to avoid log(0) during ELBO evaluation\n",
    "    prob_clip = 1e-12\n",
    "    probs = anp.clip(expit(logits), prob_clip, 1 - prob_clip)\n",
    "    log_likelihood = anp.sum(\n",
    "        y_train * anp.log(probs) \n",
    "        + (1 - y_train) * anp.log(1 - probs)\n",
    "        )\n",
    "\n",
    "    # Estimate KL divergence with Monte Carlo (variance dependent on `num_kl_samples`)\n",
    "    sigma_prior = anp.sqrt(anp.diag(cov_prior))\n",
    "\n",
    "    # Draw samples from q(w)\n",
    "    eps_samples = rng.multivariate_normal(np.zeros(D), np.eye(D), size=num_kl_samples)\n",
    "    w_samples = mu_q + eps_samples * sigma_q\n",
    "\n",
    "    # log density helper function\n",
    "    def log_diag_gaussian(x, mu, sigma):\n",
    "        return -0.5 * anp.sum(\n",
    "            ((x - mu)**2) / (sigma**2)\n",
    "            + 2 * anp.log(sigma)\n",
    "            + anp.log(2 * anp.pi),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    log_q = log_diag_gaussian(w_samples, mu_q, sigma_q)\n",
    "    log_p = log_diag_gaussian(w_samples, mu_prior, sigma_prior)\n",
    "\n",
    "    kl_mc = anp.mean(log_q - log_p)  # Monte Carlo KL estimate\n",
    "\n",
    "    return log_likelihood - kl_mc\n",
    "\n",
    "log_sigma_q_clip_range = 20.0  # Prevents overflow (e^20 is still a large std deviation but avoids infinity)\n",
    "\n",
    "# Define objective function\n",
    "def objective(params, _):\n",
    "    \"\"\"\n",
    "    Negative ELBO objective for optimisation, because Adam minimises functions.\n",
    "    \"\"\"\n",
    "    mu_q, log_cov_q = params\n",
    "    log_cov_q = anp.clip(log_cov_q, -log_sigma_q_clip_range, log_sigma_q_clip_range) # stabilise covariance\n",
    "    \n",
    "    # One reparameterisation sample per objective evaluation\n",
    "    epsilon = vi_rng.multivariate_normal(np.zeros(D), np.eye(D))\n",
    "\n",
    "    return -elbo(mu_q, log_cov_q, epsilon, vi_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.misc.optimizers import adam\n",
    "\n",
    "# Optimization Settings\n",
    "learning_rate = 0.01 # Step size for Adam updates - fairly high as Adam is adaptive and stable\n",
    "num_iterations = 4000\n",
    "elbo_values = []  # Store ELBO values for monitoring convergence\n",
    "\n",
    "# Track ELBO values during optimisation\n",
    "def track_elbo(params, *_):\n",
    "    \"\"\"\n",
    "    Callback used during optimisation to record the current ELBO value.\n",
    "    \"\"\"\n",
    "    current_elbo = -objective(params, _)\n",
    "    elbo_values.append(current_elbo)\n",
    "\n",
    "# Run Adam optimisation on variational parameters\n",
    "init_params = (mu_q, log_cov_q)\n",
    "optimized_params = adam(\n",
    "        grad(objective),\n",
    "        init_params,\n",
    "        step_size=learning_rate,\n",
    "        num_iters=num_iterations,\n",
    "        callback=track_elbo)\n",
    "\n",
    "# Unpack final values\n",
    "mu_q_vi, log_cov_q_vi = optimized_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curve to show convergence\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(elbo_values, label='ELBO')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('ELBO')\n",
    "plt.title(f'Variational Inference Training Curve')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HMC benchmark (NumPyro)\n",
    "\n",
    "Next I use JAX + NumPyro to run multi-chain HMC (NUTS) for Bayesian logistic regression, then check convergence diagnostics (including $\\hat{R}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from numpyro.diagnostics import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**HMC sampling + convergence diagnostics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import expit\n",
    "\n",
    "# Data setup (need as jax numpy arrays)\n",
    "X_train_jax = jnp.array(X_train)\n",
    "y_train_jax = jnp.array(y_train)\n",
    "\n",
    "# Logistic regression model\n",
    "def logistic_regression_model(X, y=None):\n",
    "    \"\"\"\n",
    "    Bayesian logistic regression with a Gaussian prior and Bernoulli likelihood.\n",
    "    \"\"\"\n",
    "    # Prior\n",
    "    w = numpyro.sample(\"w\", dist.Normal(0, jnp.sqrt(10)).expand([X.shape[1]]))\n",
    "    # Likelihood\n",
    "    probs = expit(jnp.dot(X, w))\n",
    "    numpyro.sample(\"y\", dist.Bernoulli(probs=probs), obs=y)\n",
    "\n",
    "# HMC setup\n",
    "num_chains = 4 # parallel chains\n",
    "num_warmup = 1000\n",
    "num_samples = 2000  # produces >=1000 effective samples after warmup\n",
    "rng_key = random.PRNGKey(0) # seed for reproducibility\n",
    "\n",
    "kernel = NUTS(logistic_regression_model) # No U-Turn Sampler (adaptive HMC)\n",
    "mcmc = MCMC(kernel, num_warmup=num_warmup, num_samples=num_samples, num_chains=num_chains)\n",
    "\n",
    "# Run HMC sampling\n",
    "mcmc.run(rng_key, X=X_train_jax, y=y_train_jax)\n",
    "mcmc.print_summary()\n",
    "\n",
    "# Get posterior samples for w\n",
    "hmc_posterior_samples = mcmc.get_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report convergence diagnostics\n",
    "summary_dict = summary(hmc_posterior_samples)\n",
    "rhat_values = [v[\"r_hat\"] for v in summary_dict.values()]\n",
    "max_rhat = max(rhat_values)\n",
    "print(f\"Max R-hat: {max_rhat:.3f}\") # confirm <= 1.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior comparison (VI vs HMC)\n",
    "\n",
    "I approximate $\\mathrm{KL}(q_\\phi \\parallel p(w \\mid D))$ by fitting a Gaussian to HMC samples and evaluating the KL with Monte Carlo samples from the VI distribution. I also compare predictive performance via test-set log-loss under both VI and HMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KL estimate + test-set log-loss comparison**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior draws from HMC\n",
    "reference_posterior = hmc_posterior_samples[\"w\"]  # shape (num_samples, D)\n",
    "\n",
    "# Fit Gaussian approximation to HMC samples\n",
    "mean_hmc = np.mean(reference_posterior, axis=0)\n",
    "cov_hmc = np.cov(reference_posterior, rowvar=False)\n",
    "\n",
    "# Match VI sample count to HMC sample count\n",
    "n_samples = reference_posterior.shape[0]\n",
    "kl_seed = 2025\n",
    "kl_rng = np.random.default_rng(kl_seed)\n",
    "\n",
    "# Reparameterisation trick\n",
    "eps = kl_rng.standard_normal((n_samples, mu_q_vi.size)) # N(0, I)\n",
    "w_vi = mu_q_vi + np.exp(0.5 * log_cov_q_vi) * eps # VI posterior samples\n",
    "\n",
    "# Compute log q_vi for each sample (diagonal Gaussian)\n",
    "def diag_gaussian_logpdf(x, mean, log_var):\n",
    "    \"\"\"\n",
    "    Log-density of a diagonal Gaussian.\n",
    "    \"\"\"\n",
    "    return -0.5 * np.sum(\n",
    "        (x - mean) ** 2 / np.exp(log_var)\n",
    "        + log_var\n",
    "        + np.log(2 * np.pi),\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "# log q(w) under VI posterior\n",
    "log_q_vi = diag_gaussian_logpdf(w_vi, mu_q_vi, log_cov_q_vi)\n",
    "\n",
    "# Compute log p(w) under Gaussian approximation to HMC posterior\n",
    "mv_hmc = multivariate_normal(mean=mean_hmc, cov=cov_hmc)\n",
    "log_p_hmc = mv_hmc.logpdf(w_vi)\n",
    "\n",
    "# Monte Carlo estimate of KL divergence\n",
    "kl = np.mean(log_q_vi - log_p_hmc)\n",
    "\n",
    "print(f\"KL Divergence (VI || HMC): {kl:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VI predictive mean (Monte Carlo)\n",
    "y_pred_vi = expit(X_test @ w_vi.T).mean(axis=1)\n",
    "\n",
    "# HMC predictive mean\n",
    "y_pred_hmc = expit(X_test @ reference_posterior.T).mean(axis=1)\n",
    "\n",
    "log_loss_vi = log_loss(y_test, y_pred_vi)\n",
    "log_loss_hmc = log_loss(y_test, y_pred_hmc)\n",
    "\n",
    "# Plot table\n",
    "print(\"Method                  |  Log-Loss\")\n",
    "print(\"------------------------|----------\")\n",
    "print(f\"Variational Inference   |  {log_loss_vi:.6f}\")\n",
    "print(f\"HMC                     |  {log_loss_hmc:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gaussian processes and Bayesian Optimisation\n",
    "\n",
    "Here I use Gaussian processes for Bayesian optimisation of the hyperparameters of a machine learning model. The dataset is taken from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php). The task is to predict electrical energy output from a combined cycle power plant; the dataset description is [here](https://archive.ics.uci.edu/dataset/294/combined+cycle+power+plant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Bayesian optimisation of Elastic net hyperparameters\n",
    "\n",
    "I use Thompson Sampling (TS) to optimise an Elastic Net regressor over the energy output dataset. The two hyperparameters are $\\alpha>0$ and $0\\le\\rho\\le1$. The function I aim to minimise is validation RMSE:\n",
    "\n",
    "$$\n",
    "\\text{RMSE}(\\alpha, \\rho) = \\sqrt{\\text{MSE}(\\mathbf{y}_\\text{val}, f(\\mathbf{x}_{\\text{val}}, \\mathcal{D}_\\text{train}, \\alpha, \\rho))}.\n",
    "$$\n",
    "\n",
    "I first load the dataset and split it into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "doq = \"https://archive.ics.uci.edu/static/public/294/combined+cycle+power+plant.zip\"\n",
    "pat_sav = \"./combined+cycle+power+plant.zip\"\n",
    "urllib.request.urlretrieve(doq, pat_sav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zip = zipfile.ZipFile('./combined+cycle+power+plant.zip', 'r')\n",
    "for name in zip.namelist():\n",
    "    zip.extract(name, '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "energy_output = pd.read_excel('./CCPP/Folds5x2_pp.xlsx','Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 9568 observations. I use a subset of $N_m$ and then split it into training/validation/test partitions (80/10/10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_m = 9000\n",
    "ndata, ncols = np.shape(energy_output)\n",
    "np.random.seed(11230)                 # Make sure you use the last five digits of your student UCard as your seed\n",
    "index = np.random.permutation(ndata)  # We permute the indexes  \n",
    "data_tot_red = energy_output.iloc[index[0:N_m], :].copy() # Select N_m points\n",
    "Ne = np.int64(np.round(0.8*N_m))    # We compute N, the number of training instances\n",
    "Neval = np.int64(np.round(0.1*N_m)) # We compute Nval, the number of validation instances   \n",
    "Netest = N_m - Ne - Neval              # We compute Ntest, the number of test instances\n",
    "index = np.random.permutation(N_m)  # We permute the indexes  \n",
    "data_training = data_tot_red.iloc[index[0:Ne], :].copy() # Select the training data\n",
    "data_val = data_tot_red.iloc[index[Ne:Ne+Neval], :].copy() # Select the validation data\n",
    "data_test = data_tot_red.iloc[index[Ne+Neval:N_m], :].copy() # Select the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xe_train = np.concatenate((np.ones((Ne,1)), (data_training.iloc[:, 0:4]).values), axis=1) \n",
    "ye_train = np.reshape((data_training.iloc[:, 4]).values, (Ne,1))\n",
    "Xe_val = np.concatenate((np.ones((Neval,1)), (data_val.iloc[:, 0:4]).values), axis=1) \n",
    "ye_val = np.reshape((data_val.iloc[:, 4]).values, (Neval,1))\n",
    "Xe_test = np.concatenate((np.ones((Netest,1)), (data_test.iloc[:, 0:4]).values), axis=1) \n",
    "ye_test = np.reshape((data_test.iloc[:, 4]).values, (Netest,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. Initial space-filling design\n",
    "\n",
    "I begin by collecting a small initial set of evaluations of the objective. I use $n_0 = 5$ initial points, producing initial observations `X0train` and `y0train`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial design + objective evaluations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale input features\n",
    "scaler_X = StandardScaler()\n",
    "Xe_train_scaled = scaler_X.fit_transform(Xe_train)\n",
    "Xe_test_scaled = scaler_X.transform(Xe_test)\n",
    "Xe_val_scaled = scaler_X.transform(Xe_val)\n",
    "\n",
    "# Number of initial samples\n",
    "n0 = 5\n",
    "\n",
    "# Sample alpha logarithmically and rho uniformly\n",
    "alpha_vals = 10 ** np.random.uniform(-4, 0, size=n0)\n",
    "rho_vals = np.random.uniform(0.0, 1.0, size=n0)\n",
    "\n",
    "# Stack to form initial design matrix\n",
    "X0train = np.vstack((alpha_vals, rho_vals)).T\n",
    "\n",
    "# Define objective function (RMSE)\n",
    "def gp_rmse(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    Compute RMSE between predictions and true values.\n",
    "    \"\"\"\n",
    "    y_pred = np.asarray(y_pred).ravel()\n",
    "    y_true = np.asarray(y_true).ravel()\n",
    "    return np.sqrt(((y_pred - y_true) ** 2).mean())\n",
    "\n",
    "# Container for initial RMSE values\n",
    "rmse_vals = np.zeros(n0)\n",
    "\n",
    "# Evaluate RMSE for each initial (alpha, rho) pair\n",
    "for i in range(n0):\n",
    "    base_model = ElasticNet(alpha=alpha_vals[i], l1_ratio=rho_vals[i], random_state=42)\n",
    "    base_model.fit(Xe_train_scaled, ye_train)\n",
    "\n",
    "    y_pred = base_model.predict(Xe_val_scaled) # Predict on validation set\n",
    "    \n",
    "    rmse_vals[i] = gp_rmse(y_pred, ye_val) # Compute and store RMSE\n",
    "\n",
    "# Initial training RMSE values\n",
    "y0train = rmse_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. Sequential decision loop (Thompson sampling)\n",
    "\n",
    "I run a Bayesian optimisation loop with a budget of 20 function evaluations. Each iteration:\n",
    "\n",
    "1. Fits a GP surrogate model to all observed data.\n",
    "\n",
    "2. Uses Thompson sampling over a grid to select the next candidate point.\n",
    "\n",
    "3. Evaluates validation RMSE at that point and appends the new observation.\n",
    "\n",
    "At the end, I extract the best observed $(\\alpha, \\rho)$ as $(\\alpha_*, \\rho_*)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GP surrogate + Thompson sampling loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "def thompson_sampling(Xtrain, ytrain, Xgrid):\n",
    "    \"\"\"\n",
    "    Thompson sampling step using an exact GP posterior over Xgrid.\n",
    "    \"\"\"\n",
    "    # Define RBF kernel\n",
    "    s_f = 1.0 # Initial values, will be optimised by GPRegressor\n",
    "    l_s = 0.5\n",
    "    kernel = s_f * RBF(length_scale=l_s,\n",
    "                        length_scale_bounds=(1e-2, 5)\n",
    "                        )\n",
    "    \n",
    "    # Fit GP on training data\n",
    "    gpr = GaussianProcessRegressor(\n",
    "        kernel=kernel,\n",
    "        normalize_y=True,\n",
    "        n_restarts_optimizer=3, # optimise s_f and l_s\n",
    "        # Add jitter for numerical stability\n",
    "        alpha=1e-4,\n",
    "        random_state=11230\n",
    "    )\n",
    "    gpr.fit(Xtrain, ytrain) # fit GP on observed evaluations\n",
    "\n",
    "    # GP posterior mean and covariance on grid\n",
    "    mu, cov = gpr.predict(Xgrid, return_cov=True)\n",
    "\n",
    "    # Draw one sample from GP posterior\n",
    "    f_sample = np.random.multivariate_normal(mean=mu, cov=cov)\n",
    "\n",
    "    # Pick minimizer of the sampled function\n",
    "    x_next = Xgrid[np.argmin(f_sample)]\n",
    "\n",
    "    return x_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Suppress ElasticNet convergence warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    category=ConvergenceWarning\n",
    ")\n",
    "\n",
    "# Parameter grid to search over/evaluate in BayesOpt/Thompson sampling\n",
    "eps = 1e-4 # Min value for rho, 0 causes convergence errors\n",
    "n_points = 30  # resolution per dimension\n",
    "\n",
    "# Again sample alpha logarithmically and rho uniformly\n",
    "alpha_vals = np.logspace(-4, 0, n_points)\n",
    "rho_vals = np.linspace(eps, 1.0, n_points)\n",
    "\n",
    "X1, X2 = np.meshgrid(alpha_vals, rho_vals, indexing=\"xy\") # Create 2D grid\n",
    "Xgrid = np.column_stack([X1.ravel(), X2.ravel()]) # Flatten to (n_points^2, 2)\n",
    "\n",
    "# Number of Thompson-sampling steps\n",
    "N_evals_bayesopt = 20\n",
    "\n",
    "def optimisation_loop():\n",
    "    \"\"\"\n",
    "    Sequential Thompson-sampling Bayesian optimisation of (alpha, rho).\n",
    "    \"\"\"\n",
    "    # Start from initial design\n",
    "    Xtrain, ytrain = X0train.copy(), y0train.copy()\n",
    "\n",
    "    for _ in range(N_evals_bayesopt):\n",
    "        # Thompson sampling proposes the next hyperparameter pair\n",
    "        x_next = thompson_sampling(Xtrain, ytrain, Xgrid)\n",
    "\n",
    "        # Train elastic net with suggested hyperparameters\n",
    "        model = ElasticNet(alpha=x_next[0], l1_ratio=x_next[1], random_state=42)\n",
    "        model.fit(Xe_train_scaled, ye_train)\n",
    "\n",
    "        # Validation RMSE acts as objective value\n",
    "        y_pred = model.predict(Xe_val_scaled)\n",
    "        y_next = gp_rmse(y_pred, ye_val)\n",
    "\n",
    "        # Append new observation to GP training data\n",
    "        Xtrain = np.vstack((Xtrain, x_next)) \n",
    "        ytrain = np.append(ytrain, y_next)\n",
    "\n",
    "    return Xtrain, ytrain\n",
    "\n",
    "# Run sequential Bayesian optimisation\n",
    "Xtrain_final, ytrain_final = optimisation_loop()\n",
    "\n",
    "# Extract best-performing parameters\n",
    "min_rmse_arg = np.argmin(ytrain_final)\n",
    "alpha_star = Xtrain_final[min_rmse_arg,0]\n",
    "rho_star = Xtrain_final[min_rmse_arg,1]\n",
    "\n",
    "print(f\"Alpha = {alpha_star:.6f}\")\n",
    "print(f\"Rho = {rho_star:.6f}\")\n",
    "print(f\"Min RMSE = {min(ytrain_final):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract sequences\n",
    "alphas = Xtrain_final[:, 0]\n",
    "rhos = Xtrain_final[:, 1]\n",
    "rmses = ytrain_final\n",
    "iters = np.arange(len(rmses))\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(3, 1, figsize=(6, 5), sharex=True)\n",
    "\n",
    "axes[0].plot(iters+1, alphas)\n",
    "axes[0].set_ylabel(\"Alpha\")\n",
    "\n",
    "axes[1].plot(iters+1, rhos)\n",
    "axes[1].set_ylabel(\"Rho (l1-ratio)\")\n",
    "\n",
    "axes[2].plot(iters+1, rmses)\n",
    "axes[2].set_ylabel(\"RMSE\")\n",
    "axes[2].set_xlabel(\"Iteration number\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Test-set performance + baseline comparison\n",
    "\n",
    "I evaluate the Elastic Net model on the test set using the hyperparameters found by Thompson-sampling Bayesian optimisation. I then compare against a straightforward alternative baseline: grid search over $(\\alpha, \\rho)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bayesian optimisation: test RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on optimal parameters\n",
    "best_model = ElasticNet(alpha=alpha_star, l1_ratio=rho_star, random_state=42)\n",
    "best_model.fit(Xe_train_scaled, ye_train)\n",
    "\n",
    "# RMSE on test set\n",
    "y_pred = best_model.predict(Xe_test_scaled)\n",
    "rmse_bayes_opt = gp_rmse(y_pred, ye_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grid search baseline: test RMSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "n_points = 50  # resolution per dimension\n",
    "alpha_vals = np.logspace(-4, -1, n_points)\n",
    "rho_vals = np.linspace(eps, 1, n_points)\n",
    "\n",
    "param_grid = {\"alpha\": alpha_vals, \"l1_ratio\": rho_vals}\n",
    "\n",
    "# Create RMSE scorer (lower is better)\n",
    "rmse_scorer = make_scorer(lambda y_pred, y_true: gp_rmse(y_pred, y_true), greater_is_better=False)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    ElasticNet(random_state=42),\n",
    "    param_grid,\n",
    "    scoring=rmse_scorer,\n",
    "    cv=3,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform gridsearch\n",
    "grid.fit(Xe_train_scaled, ye_train)\n",
    "\n",
    "# RMSE on test set\n",
    "best_model_gs = grid.best_estimator_\n",
    "y_pred_gs = best_model_gs.predict(Xe_test_scaled)\n",
    "rmse_gs = gp_rmse(y_pred_gs, ye_test)\n",
    "\n",
    "# Optimal parameters found\n",
    "alpha_gs = grid.best_params_[\"alpha\"]\n",
    "rho_gs = grid.best_params_[\"l1_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comparison\n",
    "print(\"GridSearchCV best params:\")\n",
    "print(f\"Alpha: {alpha_gs:.6f}\")\n",
    "print(f\"Rho: {rho_gs:.6f}\")\n",
    "print(f\"GridSearchCV RMSE: {rmse_gs:.6f}\")\n",
    "\n",
    "print(\"\\nBayesianOpt best params:\")\n",
    "print(f\"Alpha: {alpha_star:.6f}\")\n",
    "print(f\"Rho: {rho_star:.6f}\")\n",
    "print(f\"BayesianOpt RMSE: {rmse_bayes_opt:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "- *Finding 1.* Bayesian optimisation and grid search find differing optimal hyperparameter pairs yet yield almost identical test RMSE, indicating a flat optimum region of the objective function.\n",
    "- *Finding 2.* The Bayesian optimisation drove alpha towards zero but keeps exploring rho values with unchanging RMSE, showing regularisation strength dominates more than L1-L2 mix for predictive performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
